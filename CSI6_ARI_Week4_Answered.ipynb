{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CSI-6-ARI Week 4 Tutorial ‚Äî **COMPLETE ANSWERED VERSION**\n",
    "## Data Preprocessing\n",
    "\n",
    "This notebook contains **all code cells answered step-by-step**, including the 4 exercises.\n",
    "\n",
    "---\n",
    "### Topics covered\n",
    "1. StandardScaler (standardisation)\n",
    "2. Categorical encoding (LabelEncoder & OneHotEncoder)\n",
    "3. Train / Test split\n",
    "4. End-to-end preprocessing with MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Setup ‚Äî run this first\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Setup complete, seed =\", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 ‚Äî Standardisation with `StandardScaler`\n",
    "\n",
    "**Key formula:** `z = (x - Œº) / œÉ`  \n",
    "After standardisation every feature column has **mean ‚âà 0** and **std ‚âà 1**.\n",
    "\n",
    "This is important for distance-based (KNN, SVM, KMeans) and gradient-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "std-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "std-option1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option 1: quick one-liner ---\n",
    "# Feature matrix: 4 samples, 2 features with very different scales\n",
    "X = np.array([\n",
    "    [1,  10],\n",
    "    [3, 100],\n",
    "    [2,  55],\n",
    "    [4,  25]\n",
    "])\n",
    "\n",
    "# fit_transform: learns mean/std from X and applies z = (x-mean)/std\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "print(\"Standardised X (2 features):\")\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "std-option2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option 2: using sklearn.preprocessing directly (3 features) ---\n",
    "import sklearn\n",
    "\n",
    "X3 = np.array([\n",
    "    [1,  10,  0],\n",
    "    [3, 100, 10],\n",
    "    [2,  55,  5],\n",
    "    [4,  25,  2]\n",
    "])\n",
    "\n",
    "X3_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X3)\n",
    "print(\"Standardised X (3 features):\")\n",
    "print(X3_scaled)\n",
    "# Notice each column is now on the same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-md",
   "metadata": {},
   "source": [
    "---\n",
    "### üìù Exercise 1 ‚Äî Check standardisation\n",
    "\n",
    "**Task:** Create `X2` (shape 5√ó2) with very different column scales, standardise it, then verify mean‚âà0 and std‚âà1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXERCISE 1 ‚Äî ANSWER\n",
    "\n",
    "# Step 1: Create X2 with two features on very different scales\n",
    "X2 = np.array([\n",
    "    [1,    10],\n",
    "    [2,    50],\n",
    "    [3,   100],\n",
    "    [4,    25],\n",
    "    [5,    60],\n",
    "])\n",
    "print(\"Original X2:\")\n",
    "print(X2)\n",
    "\n",
    "# Step 2: Standardise using StandardScaler\n",
    "X2_scaled = StandardScaler().fit_transform(X2)\n",
    "print(\"\\nScaled X2:\")\n",
    "print(X2_scaled)\n",
    "\n",
    "# Step 3: Compute per-column mean and std\n",
    "col_means = X2_scaled.mean(axis=0)\n",
    "col_stds  = X2_scaled.std(axis=0)\n",
    "\n",
    "print(\"\\nColumn means (should be ~0):\", col_means)\n",
    "print(\"Column stds  (should be ~1):\", col_stds)\n",
    "\n",
    "# Explanation:\n",
    "# StandardScaler standardises each column independently.\n",
    "# Tiny non-zero means (e.g. 1e-16) are just floating-point rounding errors, not actual bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 ‚Äî Handling Categorical Variables (Encoding)\n",
    "\n",
    "Machine learning models need numbers. Two strategies:\n",
    "- **Label Encoding**: assigns integers (0, 1, 2, ‚Ä¶). Implies ordering ‚Äî OK for ordinal categories.\n",
    "- **One-Hot Encoding**: creates a binary column per category. No ordering implied ‚Äî best for nominal categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cat-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original categorical feature matrix\n",
    "X_cat = np.array([\n",
    "    [\"Red\",   \"Petrol\", \"Sedan\"],\n",
    "    [\"Black\", \"Diesel\", \"Sedan\"],\n",
    "    [\"Blue\",  \"Diesel\", \"Hatchback\"]\n",
    "])\n",
    "print(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "label-enc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode the 'Colour' feature ‚Äî assigns integers alphabetically\n",
    "encoded_colours = encoder.fit_transform([\"Red\", \"Black\", \"Blue\"])\n",
    "print(\"Label encoded ['Red','Black','Blue']:\", encoded_colours)\n",
    "# Black=0, Blue=1, Red=2  (alphabetical order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "label-enc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding with a new value 'Yellow'\n",
    "encoded2 = encoder.fit_transform([\"Red\", \"Black\", \"Blue\", \"Yellow\", \"Red\"])\n",
    "print(\"Label encoded with Yellow:\", encoded2)\n",
    "# Black=0, Blue=1, Red=2, Yellow=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ohe-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame used in this part\n",
    "df = pd.DataFrame({\n",
    "    'Colour':    ['Red', 'Black', 'Blue'],\n",
    "    'Fuel Type': ['Petrol', 'Diesel', 'Diesel'],\n",
    "    'Body':      ['Sedan', 'Sedan', 'Hatchback']\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ohe-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Fit OneHotEncoder on the 'Colour' column\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "ohe.fit(df[['Colour']])\n",
    "print(\"Unique colour categories:\", ohe.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ohe-transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform ‚Äî produces a sparse matrix; .toarray() makes it dense\n",
    "ohe_result = ohe.transform(df[['Colour']]).toarray()\n",
    "print(\"One-hot encoded colours:\")\n",
    "print(ohe_result)\n",
    "# Columns order: Black, Blue, Red\n",
    "# Row 0 (Red)  ‚Üí [0, 0, 1]\n",
    "# Row 1 (Black)‚Üí [1, 0, 0]\n",
    "# Row 2 (Blue) ‚Üí [0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ohe-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put results into a readable DataFrame\n",
    "new_df = pd.DataFrame(\n",
    "    ohe_result.astype(int),\n",
    "    columns=['Black', 'Blue', 'Red']\n",
    ")\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-md",
   "metadata": {},
   "source": [
    "---\n",
    "### üìù Exercise 2 ‚Äî Label encoding vs One-hot encoding\n",
    "\n",
    "**Task:** Apply both encodings to the `Fuel Type` column, then explain when one-hot is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXERCISE 2 ‚Äî ANSWER\n",
    "\n",
    "# Work on a copy to avoid mutating the original df\n",
    "df_tmp = df.copy()\n",
    "\n",
    "# --- Part 1: Label encoding for 'Fuel Type' ---\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_tmp[\"FuelType_LE\"] = le.fit_transform(df_tmp[\"Fuel Type\"])\n",
    "print(\"DataFrame with Label Encoded Fuel Type:\")\n",
    "print(df_tmp)\n",
    "print(\"\\nEncoding mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "# Diesel=0, Petrol=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-ohe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 2: One-hot encoding for 'Fuel Type' using pd.get_dummies ---\n",
    "fuel_ohe = pd.get_dummies(df_tmp[\"Fuel Type\"], prefix=\"Fuel\")\n",
    "print(\"One-hot encoded Fuel Type:\")\n",
    "print(fuel_ohe)\n",
    "\n",
    "# Combine with original (drop the original 'Fuel Type' column)\n",
    "combined = pd.concat([df_tmp.drop(columns=[\"Fuel Type\", \"FuelType_LE\"]), fuel_ohe], axis=1)\n",
    "print(\"\\nCombined DataFrame:\")\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-explain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 3: Explanation ---\n",
    "explanation = \"\"\"\n",
    "When to prefer One-Hot Encoding over Label Encoding:\n",
    "\n",
    "1. One-hot encoding should be used when the categorical variable is NOMINAL \n",
    "   (no natural ordering), such as fuel type (Diesel, Petrol, Electric). \n",
    "   Label encoding would incorrectly imply Diesel < Petrol, which has no meaning.\n",
    "\n",
    "2. Most ML algorithms (logistic regression, SVM, neural networks) treat \n",
    "   integer-encoded labels as numeric distances, leading to biased predictions.\n",
    "   One-hot avoids this by treating each category as an independent binary feature.\n",
    "\n",
    "3. However, if there are MANY unique categories (high cardinality), one-hot \n",
    "   encoding creates very wide sparse matrices, so label encoding or target \n",
    "   encoding may be more practical in those cases.\n",
    "\"\"\"\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 ‚Äî Train / Test Split\n",
    "\n",
    "`train_test_split` randomly divides data into training and test sets.  \n",
    "- `test_size=0.2` ‚Üí 80% train, 20% test  \n",
    "- Always set `random_state` for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset with 10 samples: 2 numeric features + 1 categorical feature\n",
    "X = ([12,300,'Red'],[11,280,'Red'],[15,264,'Black'],[9,230,'Blue'],\n",
    "     [25,459,'Black'],[12,400,'Red'],[42,355,'Blue'],[32,435,'Red'],\n",
    "     [22,564,'Black'],[21,231,'Red'])\n",
    "\n",
    "# Target labels\n",
    "y = [1,2,1,1,1,2,2,1,2,1]\n",
    "\n",
    "print(\"Full X:\", X)\n",
    "print(\"Full y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: 80% train, 20% test, fixed seed for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=4\n",
    ")\n",
    "\n",
    "print(\"X_train (8 samples):\", X_train)\n",
    "print(\"\\nX_test  (2 samples):\", X_test)\n",
    "print(\"\\ny_train:\", y_train)\n",
    "print(\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts-note",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test size (20%) ‚Üí 2 out of 10 samples go to test, 8 to train.\n",
    "# random_state=4 ensures the same split every time you run the notebook.\n",
    "print(\"Train size:\", len(X_train), \"| Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-md",
   "metadata": {},
   "source": [
    "---\n",
    "### üìù Exercise 3 ‚Äî Inspect your split\n",
    "\n",
    "**Task:** Re-split with `test_size=0.3`, print sizes, count 1s and 2s in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXERCISE 3 ‚Äî ANSWER\n",
    "\n",
    "# Step 1: Split with test_size=0.3 (30% test, 70% train)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=4\n",
    ")\n",
    "\n",
    "# Step 2: Print sizes\n",
    "print(\"=== Sizes ===\")\n",
    "print(f\"X_train: {len(X_train2)} samples\")\n",
    "print(f\"X_test:  {len(X_test2)} samples\")\n",
    "print(f\"y_train: {len(y_train2)} labels\")\n",
    "print(f\"y_test:  {len(y_test2)} labels\")\n",
    "\n",
    "# Step 3: Count class labels\n",
    "n1_train = y_train2.count(1)\n",
    "n2_train = y_train2.count(2)\n",
    "n1_test  = y_test2.count(1)\n",
    "n2_test  = y_test2.count(2)\n",
    "\n",
    "print(\"\\n=== Class Counts ===\")\n",
    "print(f\"y_train  ‚Üí class 1: {n1_train}, class 2: {n2_train}\")\n",
    "print(f\"y_test   ‚Üí class 1: {n1_test},  class 2: {n2_test}\")\n",
    "\n",
    "# NOTE: Because the dataset is very small (10 samples), class balance\n",
    "# can shift noticeably between splits. In practice, use stratify=y\n",
    "# to preserve the original class proportions in both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4 ‚Äî End-to-End Preprocessing (MinMaxScaler)\n",
    "\n",
    "**MinMaxScaler** rescales features to a fixed range (default [0, 1]):  \n",
    "`x_scaled = (x - x_min) / (x_max - x_min)`\n",
    "\n",
    "**‚ö†Ô∏è Key rule ‚Äî fit on train only:**  \n",
    "Compute `x_min` and `x_max` from `X_train`, then apply to both `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4-create-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fake_reg.csv dataset (gem stone measurements)\n",
    "rng = np.random.default_rng(SEED)\n",
    "n = 1000\n",
    "feature1 = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "feature2 = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "noise    = rng.normal(loc=0.0, scale=0.15, size=n)\n",
    "price    = 50_000 + 12_000*feature1 + 8_000*feature2 + 10_000*noise\n",
    "\n",
    "fake = pd.DataFrame({\"feature1\": feature1, \"feature2\": feature2, \"price\": price})\n",
    "fake.to_csv(\"fake_reg.csv\", index=False)\n",
    "print(\"Created fake_reg.csv with shape\", fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('fake_reg.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) and target (y)\n",
    "X = df[['feature1', 'feature2']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Split 70% train / 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)   # (700, 2)\n",
    "print(\"X_test  shape:\", X_test.shape)    # (300, 2)\n",
    "print(\"y_train shape:\", y_train.shape)   # (700,)\n",
    "print(\"y_test  shape:\", y_test.shape)    # (300,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# CRITICAL: fit ONLY on training data (no data leakage)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both sets using the training statistics\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train after MinMaxScaler (first 3 rows):\")\n",
    "print(X_train[:3])\n",
    "print(\"\\nX_test after MinMaxScaler (first 3 rows):\")\n",
    "print(X_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-md",
   "metadata": {},
   "source": [
    "---\n",
    "### üìù Exercise 4 ‚Äî Check for data leakage\n",
    "\n",
    "**Task:** Verify `X_train` values are in [0,1], check `X_test` range, explain any out-of-range values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXERCISE 4 ‚Äî ANSWER\n",
    "\n",
    "# Step 1: Min and max of X_train after scaling\n",
    "print(\"=== X_train (scaled) ===\")\n",
    "print(\"Per-column min:\", X_train.min(axis=0))   # Should both be exactly 0.0\n",
    "print(\"Per-column max:\", X_train.max(axis=0))   # Should both be exactly 1.0\n",
    "\n",
    "# Step 2: Min and max of X_test after scaling\n",
    "print(\"\\n=== X_test (scaled) ===\")\n",
    "print(\"Per-column min:\", X_test.min(axis=0))    # Could be slightly < 0\n",
    "print(\"Per-column max:\", X_test.max(axis=0))    # Could be slightly > 1\n",
    "\n",
    "# Step 3: Check if any test values fall outside [0, 1]\n",
    "outside = ((X_test < 0) | (X_test > 1)).any()\n",
    "print(\"\\nAny X_test values outside [0, 1]?\", outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-explain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Explanation ---\n",
    "explanation = \"\"\"\n",
    "Why can X_test values fall outside [0, 1]?\n",
    "\n",
    "1. MinMaxScaler learns x_min and x_max exclusively from X_train.\n",
    "   It scales values using: (x - train_min) / (train_max - train_min)\n",
    "\n",
    "2. If X_test contains values BELOW the training minimum, the scaled \n",
    "   value will be negative (< 0). If X_test values are ABOVE the \n",
    "   training maximum, the scaled value will exceed 1.\n",
    "\n",
    "3. This is CORRECT BEHAVIOUR and not a bug. The alternative ‚Äî fitting \n",
    "   the scaler on the full dataset ‚Äî would leak test-set information \n",
    "   into the preprocessing step, making model evaluation unreliable.\n",
    "   Slight out-of-range values on test data are acceptable.\n",
    "\"\"\"\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Summary ‚Äî Key takeaways from Week 4\n",
    "\n",
    "| Concept | Tool | When to use |\n",
    "|---|---|---|\n",
    "| Standardisation | `StandardScaler` | KNN, SVM, PCA, gradient descent models |\n",
    "| Normalisation | `MinMaxScaler` | Neural networks, when bounded range needed |\n",
    "| Label encoding | `LabelEncoder` | Ordinal categories, tree models |\n",
    "| One-hot encoding | `OneHotEncoder` / `pd.get_dummies` | Nominal categories, linear/distance models |\n",
    "| Train/test split | `train_test_split` | Always ‚Äî before any fitting! |\n",
    "\n",
    "### ‚ö†Ô∏è The golden rule\n",
    "**Fit preprocessing (scalers, encoders) on the training set only**, then transform both sets. Fitting on the full dataset causes **data leakage** and gives unreliable model evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
