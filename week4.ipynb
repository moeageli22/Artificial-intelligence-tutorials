{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CSI-6-ARI Week 4 Tutorial — **COMPLETE ANSWERED VERSION**\n",
    "## Data Preprocessing\n",
    "\n",
    "This notebook contains **all code cells answered step-by-step**, including the 4 exercises.\n",
    "\n",
    "---\n",
    "### Topics covered\n",
    "1. StandardScaler (standardisation)\n",
    "2. Categorical encoding (LabelEncoder & OneHotEncoder)\n",
    "3. Train / Test split\n",
    "4. End-to-end preprocessing with MinMaxScaler\n"
   ],
   "id": "6a6f2813c7237d44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set up",
   "id": "39fb7d86dc5cbe38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T15:14:42.768994Z",
     "start_time": "2026-02-18T15:14:38.762946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Setup complete, seed =\", SEED)"
   ],
   "id": "d95818969c2de72d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete, seed = 42\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1 — Standardisation with `StandardScaler`\n",
    "\n",
    "**Key formula:** `z = (x - μ) / σ`\n",
    "After standardisation every feature column has **mean ≈ 0** and **std ≈ 1**.\n",
    "\n",
    "This is important for distance-based (KNN, SVM, KMeans) and gradient-based models."
   ],
   "id": "596bf1350c6a4f78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T15:16:37.393997Z",
     "start_time": "2026-02-18T15:15:54.407503Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.preprocessing import StandardScaler",
   "id": "bf1eacb5672c346f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T15:16:37.409798Z",
     "start_time": "2026-02-18T15:16:37.406489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Option 1: quick one-liner ---\n",
    "# Feature matrix: 4 samples, 2 features with very different scales\n",
    "X = np.array([\n",
    "    [1,  10],\n",
    "    [3, 100],\n",
    "    [2,  55],\n",
    "    [4,  25]\n",
    "])\n",
    "\n",
    "# fit_transform: learns mean/std from X and applies z = (x-mean)/std\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "print(\"Standardised X (2 features):\")\n",
    "print(X_scaled)"
   ],
   "id": "a6e16e107719f800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardised X (2 features):\n",
      "[[-1.34164079 -1.09108945]\n",
      " [ 0.4472136   1.52752523]\n",
      " [-0.4472136   0.21821789]\n",
      " [ 1.34164079 -0.65465367]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Option 2: using sklearn.preprocessing directly (3 features) ---\n",
    "import sklearn\n",
    "\n",
    "X3 = np.array([\n",
    "    [1,  10,  0],\n",
    "    [3, 100, 10],\n",
    "    [2,  55,  5],\n",
    "    [4,  25,  2]\n",
    "])\n",
    "\n",
    "X3_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X3)\n",
    "print(\"Standardised X (3 features):\")\n",
    "print(X3_scaled)\n",
    "# Notice each column is now on the same scale"
   ],
   "id": "b63c9576ad00dae4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Exercise 1 — Check standardisation\n",
    "\n",
    "**Task:** Create `X2` (shape 5×2) with very different column scales, standardise it, then verify mean≈0 and std≈1."
   ],
   "id": "cbc1a8e7c83411fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: Create X2 with two features on very different scales\n",
    "X2 = np.array([\n",
    "    [1,    10],\n",
    "    [2,    50],\n",
    "    [3,   100],\n",
    "    [4,    25],\n",
    "    [5,    60],\n",
    "])\n",
    "print(\"Original X2:\")\n",
    "print(X2)\n",
    "\n",
    "# Step 2: Standardise using StandardScaler\n",
    "X2_scaled = StandardScaler().fit_transform(X2)\n",
    "print(\"\\nScaled X2:\")\n",
    "print(X2_scaled)\n",
    "\n",
    "# Step 3: Compute per-column mean and std\n",
    "col_means = X2_scaled.mean(axis=0)\n",
    "col_stds  = X2_scaled.std(axis=0)\n",
    "\n",
    "print(\"\\nColumn means (should be ~0):\", col_means)\n",
    "print(\"Column stds  (should be ~1):\", col_stds)\n",
    "\n",
    "# Explanation:\n",
    "# StandardScaler standardises each column independently.\n",
    "# Tiny non-zero means (e.g. 1e-16) are just floating-point rounding errors, not actual bias."
   ],
   "id": "7fffc37668d4310f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 2 — Handling Categorical Variables (Encoding)\n",
    "\n",
    "Machine learning models need numbers. Two strategies:\n",
    "- **Label Encoding**: assigns integers (0, 1, 2, …). Implies ordering — OK for ordinal categories.\n",
    "- **One-Hot Encoding**: creates a binary column per category. No ordering implied — best for nominal categories."
   ],
   "id": "3b800600e1a6286c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Original categorical feature matrix\n",
    "X_cat = np.array([\n",
    "    [\"Red\",   \"Petrol\", \"Sedan\"],\n",
    "    [\"Black\", \"Diesel\", \"Sedan\"],\n",
    "    [\"Blue\",  \"Diesel\", \"Hatchback\"]\n",
    "])\n",
    "print(X_cat)"
   ],
   "id": "69d091286ae807b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode the 'Colour' feature — assigns integers alphabetically\n",
    "encoded_colours = encoder.fit_transform([\"Red\", \"Black\", \"Blue\"])\n",
    "print(\"Label encoded ['Red','Black','Blue']:\", encoded_colours)\n",
    "# Black=0, Blue=1, Red=2  (alphabetical order)"
   ],
   "id": "3ba4dbd789a44643"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encoding with a new value 'Yellow'\n",
    "encoded2 = encoder.fit_transform([\"Red\", \"Black\", \"Blue\", \"Yellow\", \"Red\"])\n",
    "print(\"Label encoded with Yellow:\", encoded2)\n",
    "# Black=0, Blue=1, Red=2, Yellow=3"
   ],
   "id": "ccb0ab978aa904d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create the DataFrame used in this part\n",
    "df = pd.DataFrame({\n",
    "    'Colour':    ['Red', 'Black', 'Blue'],\n",
    "    'Fuel Type': ['Petrol', 'Diesel', 'Diesel'],\n",
    "    'Body':      ['Sedan', 'Sedan', 'Hatchback']\n",
    "})\n",
    "print(df)"
   ],
   "id": "59803742909121ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Fit OneHotEncoder on the 'Colour' column\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "ohe.fit(df[['Colour']])\n",
    "print(\"Unique colour categories:\", ohe.categories_)"
   ],
   "id": "9a9c6d1431883a21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Transform — produces a sparse matrix; .toarray() makes it dense\n",
    "ohe_result = ohe.transform(df[['Colour']]).toarray()\n",
    "print(\"One-hot encoded colours:\")\n",
    "print(ohe_result)\n",
    "# Columns order: Black, Blue, Red\n",
    "# Row 0 (Red)  → [0, 0, 1]\n",
    "# Row 1 (Black)→ [1, 0, 0]\n",
    "# Row 2 (Blue) → [0, 1, 0]"
   ],
   "id": "5aec1afba9634784"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "###  Exercise 2 — Label encoding vs One-hot encoding\n",
    "\n",
    "**Task:** Apply both encodings to the `Fuel Type` column, then explain when one-hot is preferred."
   ],
   "id": "dfa6d62986fa4f9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Work on a copy to avoid mutating the original df\n",
    "df_tmp = df.copy()\n",
    "\n",
    "# --- Part 1: Label encoding for 'Fuel Type' ---\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_tmp[\"FuelType_LE\"] = le.fit_transform(df_tmp[\"Fuel Type\"])\n",
    "print(\"DataFrame with Label Encoded Fuel Type:\")\n",
    "print(df_tmp)\n",
    "print(\"\\nEncoding mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "# Diesel=0, Petrol=1"
   ],
   "id": "2fb4bc1a573cee74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Part 2: One-hot encoding for 'Fuel Type' using pd.get_dummies ---\n",
    "fuel_ohe = pd.get_dummies(df_tmp[\"Fuel Type\"], prefix=\"Fuel\")\n",
    "print(\"One-hot encoded Fuel Type:\")\n",
    "print(fuel_ohe)\n",
    "\n",
    "# Combine with original (drop the original 'Fuel Type' column)\n",
    "combined = pd.concat([df_tmp.drop(columns=[\"Fuel Type\", \"FuelType_LE\"]), fuel_ohe], axis=1)\n",
    "print(\"\\nCombined DataFrame:\")\n",
    "print(combined)"
   ],
   "id": "9b9060aa83ab5a6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Part 3: Explanation ---\n",
    "explanation = \"\"\"\n",
    "When to prefer One-Hot Encoding over Label Encoding:\n",
    "\n",
    "1. One-hot encoding should be used when the categorical variable is NOMINAL\n",
    "   (no natural ordering), such as fuel type (Diesel, Petrol, Electric).\n",
    "   Label encoding would incorrectly imply Diesel < Petrol, which has no meaning.\n",
    "\n",
    "2. Most ML algorithms (logistic regression, SVM, neural networks) treat\n",
    "   integer-encoded labels as numeric distances, leading to biased predictions.\n",
    "   One-hot avoids this by treating each category as an independent binary feature.\n",
    "\n",
    "3. However, if there are MANY unique categories (high cardinality), one-hot\n",
    "   encoding creates very wide sparse matrices, so label encoding or target\n",
    "   encoding may be more practical in those cases.\n",
    "\"\"\"\n",
    "print(explanation)"
   ],
   "id": "9d5b9026952ab226"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
