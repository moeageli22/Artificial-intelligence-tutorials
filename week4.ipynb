{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CSI-6-ARI Week 4 Tutorial — **COMPLETE ANSWERED VERSION**\n",
    "## Data Preprocessing\n",
    "\n",
    "This notebook contains **all code cells answered step-by-step**, including the 4 exercises.\n",
    "\n",
    "---\n",
    "### Topics covered\n",
    "1. StandardScaler (standardisation)\n",
    "2. Categorical encoding (LabelEncoder & OneHotEncoder)\n",
    "3. Train / Test split\n",
    "4. End-to-end preprocessing with MinMaxScaler\n"
   ],
   "id": "6a6f2813c7237d44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set up",
   "id": "39fb7d86dc5cbe38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:17.593636Z",
     "start_time": "2026-02-19T14:40:16.100410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Setup complete, seed =\", SEED)"
   ],
   "id": "d95818969c2de72d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete, seed = 42\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1 — Standardisation with `StandardScaler`\n",
    "\n",
    "**Key formula:** `z = (x - μ) / σ`\n",
    "After standardisation every feature column has **mean ≈ 0** and **std ≈ 1**.\n",
    "\n",
    "This is important for distance-based (KNN, SVM, KMeans) and gradient-based models."
   ],
   "id": "596bf1350c6a4f78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:19.607384Z",
     "start_time": "2026-02-19T14:40:17.652667Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.preprocessing import StandardScaler",
   "id": "bf1eacb5672c346f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:19.733223Z",
     "start_time": "2026-02-19T14:40:19.723665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Option 1: quick one-liner ---\n",
    "# Feature matrix: 4 samples, 2 features with very different scales\n",
    "X = np.array([\n",
    "    [1,  10],\n",
    "    [3, 100],\n",
    "    [2,  55],\n",
    "    [4,  25]\n",
    "])\n",
    "\n",
    "# fit_transform: learns mean/std from X and applies z = (x-mean)/std\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "print(\"Standardised X (2 features):\")\n",
    "print(X_scaled)"
   ],
   "id": "a6e16e107719f800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardised X (2 features):\n",
      "[[-1.34164079 -1.09108945]\n",
      " [ 0.4472136   1.52752523]\n",
      " [-0.4472136   0.21821789]\n",
      " [ 1.34164079 -0.65465367]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:19.888781Z",
     "start_time": "2026-02-19T14:40:19.882204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Option 2: using sklearn.preprocessing directly (3 features) ---\n",
    "import sklearn\n",
    "\n",
    "X3 = np.array([\n",
    "    [1,  10,  0],\n",
    "    [3, 100, 10],\n",
    "    [2,  55,  5],\n",
    "    [4,  25,  2]\n",
    "])\n",
    "\n",
    "X3_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X3)\n",
    "print(\"Standardised X (3 features):\")\n",
    "print(X3_scaled)\n",
    "# Notice each column is now on the same scale"
   ],
   "id": "b63c9576ad00dae4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardised X (3 features):\n",
      "[[-1.34164079 -1.09108945 -1.12832963]\n",
      " [ 0.4472136   1.52752523  1.52656362]\n",
      " [-0.4472136   0.21821789  0.19911699]\n",
      " [ 1.34164079 -0.65465367 -0.59735098]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Exercise 1 — Check standardisation\n",
    "\n",
    "**Task:** Create `X2` (shape 5×2) with very different column scales, standardise it, then verify mean≈0 and std≈1."
   ],
   "id": "cbc1a8e7c83411fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:19.990113Z",
     "start_time": "2026-02-19T14:40:19.977737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Create X2 with two features on very different scales\n",
    "X2 = np.array([\n",
    "    [1,    10],\n",
    "    [2,    50],\n",
    "    [3,   100],\n",
    "    [4,    25],\n",
    "    [5,    60],\n",
    "])\n",
    "print(\"Original X2:\")\n",
    "print(X2)\n",
    "\n",
    "# Step 2: Standardise using StandardScaler\n",
    "X2_scaled = StandardScaler().fit_transform(X2)\n",
    "print(\"\\nScaled X2:\")\n",
    "print(X2_scaled)\n",
    "\n",
    "# Step 3: Compute per-column mean and std\n",
    "col_means = X2_scaled.mean(axis=0)\n",
    "col_stds  = X2_scaled.std(axis=0)\n",
    "\n",
    "print(\"\\nColumn means (should be ~0):\", col_means)\n",
    "print(\"Column stds  (should be ~1):\", col_stds)\n",
    "\n",
    "# Explanation:\n",
    "# StandardScaler standardises each column independently.\n",
    "# Tiny non-zero means (e.g. 1e-16) are just floating-point rounding errors, not actual bias."
   ],
   "id": "7fffc37668d4310f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X2:\n",
      "[[  1  10]\n",
      " [  2  50]\n",
      " [  3 100]\n",
      " [  4  25]\n",
      " [  5  60]]\n",
      "\n",
      "Scaled X2:\n",
      "[[-1.41421356 -1.25610542]\n",
      " [-0.70710678  0.03220783]\n",
      " [ 0.          1.6425994 ]\n",
      " [ 0.70710678 -0.77298795]\n",
      " [ 1.41421356  0.35428614]]\n",
      "\n",
      "Column means (should be ~0): [0.00000000e+00 3.33066907e-17]\n",
      "Column stds  (should be ~1): [1. 1.]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 2 — Handling Categorical Variables (Encoding)\n",
    "\n",
    "Machine learning models need numbers. Two strategies:\n",
    "- **Label Encoding**: assigns integers (0, 1, 2, …). Implies ordering — OK for ordinal categories.\n",
    "- **One-Hot Encoding**: creates a binary column per category. No ordering implied — best for nominal categories."
   ],
   "id": "3b800600e1a6286c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:20.064786Z",
     "start_time": "2026-02-19T14:40:20.060274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Original categorical feature matrix\n",
    "X_cat = np.array([\n",
    "    [\"Red\",   \"Petrol\", \"Sedan\"],\n",
    "    [\"Black\", \"Diesel\", \"Sedan\"],\n",
    "    [\"Blue\",  \"Diesel\", \"Hatchback\"]\n",
    "])\n",
    "print(X_cat)"
   ],
   "id": "69d091286ae807b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Red' 'Petrol' 'Sedan']\n",
      " ['Black' 'Diesel' 'Sedan']\n",
      " ['Blue' 'Diesel' 'Hatchback']]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:20.108221Z",
     "start_time": "2026-02-19T14:40:20.102866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode the 'Colour' feature — assigns integers alphabetically\n",
    "encoded_colours = encoder.fit_transform([\"Red\", \"Black\", \"Blue\"])\n",
    "print(\"Label encoded ['Red','Black','Blue']:\", encoded_colours)\n",
    "# Black=0, Blue=1, Red=2  (alphabetical order)"
   ],
   "id": "3ba4dbd789a44643",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoded ['Red','Black','Blue']: [2 0 1]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:20.151244Z",
     "start_time": "2026-02-19T14:40:20.147485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encoding with a new value 'Yellow'\n",
    "encoded2 = encoder.fit_transform([\"Red\", \"Black\", \"Blue\", \"Yellow\", \"Red\"])\n",
    "print(\"Label encoded with Yellow:\", encoded2)\n",
    "# Black=0, Blue=1, Red=2, Yellow=3"
   ],
   "id": "ccb0ab978aa904d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoded with Yellow: [2 0 1 3 2]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:20.226702Z",
     "start_time": "2026-02-19T14:40:20.196766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the DataFrame used in this part\n",
    "df = pd.DataFrame({\n",
    "    'Colour':    ['Red', 'Black', 'Blue'],\n",
    "    'Fuel Type': ['Petrol', 'Diesel', 'Diesel'],\n",
    "    'Body':      ['Sedan', 'Sedan', 'Hatchback']\n",
    "})\n",
    "print(df)"
   ],
   "id": "59803742909121ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Colour Fuel Type       Body\n",
      "0    Red    Petrol      Sedan\n",
      "1  Black    Diesel      Sedan\n",
      "2   Blue    Diesel  Hatchback\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:20.302356Z",
     "start_time": "2026-02-19T14:40:20.275700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Fit OneHotEncoder on the 'Colour' column\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "ohe.fit(df[['Colour']])\n",
    "print(\"Unique colour categories:\", ohe.categories_)"
   ],
   "id": "9a9c6d1431883a21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique colour categories: [array(['Black', 'Blue', 'Red'], dtype=object)]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:20.371579Z",
     "start_time": "2026-02-19T14:40:20.362853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transform — produces a sparse matrix; .toarray() makes it dense\n",
    "ohe_result = ohe.transform(df[['Colour']]).toarray()\n",
    "print(\"One-hot encoded colours:\")\n",
    "print(ohe_result)\n",
    "# Columns order: Black, Blue, Red\n",
    "# Row 0 (Red)  → [0, 0, 1]\n",
    "# Row 1 (Black)→ [1, 0, 0]\n",
    "# Row 2 (Blue) → [0, 1, 0]"
   ],
   "id": "5aec1afba9634784",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded colours:\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "###  Exercise 2 — Label encoding vs One-hot encoding\n",
    "\n",
    "**Task:** Apply both encodings to the `Fuel Type` column, then explain when one-hot is preferred."
   ],
   "id": "dfa6d62986fa4f9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:20.389813Z",
     "start_time": "2026-02-19T14:40:20.381093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Work on a copy to avoid mutating the original df\n",
    "df_tmp = df.copy()\n",
    "\n",
    "# --- Part 1: Label encoding for 'Fuel Type' ---\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_tmp[\"FuelType_LE\"] = le.fit_transform(df_tmp[\"Fuel Type\"])\n",
    "print(\"DataFrame with Label Encoded Fuel Type:\")\n",
    "print(df_tmp)\n",
    "print(\"\\nEncoding mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "# Diesel=0, Petrol=1"
   ],
   "id": "2fb4bc1a573cee74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Label Encoded Fuel Type:\n",
      "  Colour Fuel Type       Body  FuelType_LE\n",
      "0    Red    Petrol      Sedan            1\n",
      "1  Black    Diesel      Sedan            0\n",
      "2   Blue    Diesel  Hatchback            0\n",
      "\n",
      "Encoding mapping: {'Diesel': 0, 'Petrol': 1}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:20.448814Z",
     "start_time": "2026-02-19T14:40:20.432545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Part 2: One-hot encoding for 'Fuel Type' using pd.get_dummies ---\n",
    "fuel_ohe = pd.get_dummies(df_tmp[\"Fuel Type\"], prefix=\"Fuel\")\n",
    "print(\"One-hot encoded Fuel Type:\")\n",
    "print(fuel_ohe)\n",
    "\n",
    "# Combine with original (drop the original 'Fuel Type' column)\n",
    "combined = pd.concat([df_tmp.drop(columns=[\"Fuel Type\", \"FuelType_LE\"]), fuel_ohe], axis=1)\n",
    "print(\"\\nCombined DataFrame:\")\n",
    "print(combined)"
   ],
   "id": "9b9060aa83ab5a6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded Fuel Type:\n",
      "   Fuel_Diesel  Fuel_Petrol\n",
      "0        False         True\n",
      "1         True        False\n",
      "2         True        False\n",
      "\n",
      "Combined DataFrame:\n",
      "  Colour       Body  Fuel_Diesel  Fuel_Petrol\n",
      "0    Red      Sedan        False         True\n",
      "1  Black      Sedan         True        False\n",
      "2   Blue  Hatchback         True        False\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:20.555230Z",
     "start_time": "2026-02-19T14:40:20.549138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Part 3: Explanation ---\n",
    "explanation = \"\"\"\n",
    "When to prefer One-Hot Encoding over Label Encoding:\n",
    "\n",
    "1. One-hot encoding should be used when the categorical variable is NOMINAL\n",
    "   (no natural ordering), such as fuel type (Diesel, Petrol, Electric).\n",
    "   Label encoding would incorrectly imply Diesel < Petrol, which has no meaning.\n",
    "\n",
    "2. Most ML algorithms (logistic regression, SVM, neural networks) treat\n",
    "   integer-encoded labels as numeric distances, leading to biased predictions.\n",
    "   One-hot avoids this by treating each category as an independent binary feature.\n",
    "\n",
    "3. However, if there are MANY unique categories (high cardinality), one-hot\n",
    "   encoding creates very wide sparse matrices, so label encoding or target\n",
    "   encoding may be more practical in those cases.\n",
    "\"\"\"\n",
    "print(explanation)"
   ],
   "id": "9d5b9026952ab226",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When to prefer One-Hot Encoding over Label Encoding:\n",
      "\n",
      "1. One-hot encoding should be used when the categorical variable is NOMINAL\n",
      "   (no natural ordering), such as fuel type (Diesel, Petrol, Electric).\n",
      "   Label encoding would incorrectly imply Diesel < Petrol, which has no meaning.\n",
      "\n",
      "2. Most ML algorithms (logistic regression, SVM, neural networks) treat\n",
      "   integer-encoded labels as numeric distances, leading to biased predictions.\n",
      "   One-hot avoids this by treating each category as an independent binary feature.\n",
      "\n",
      "3. However, if there are MANY unique categories (high cardinality), one-hot\n",
      "   encoding creates very wide sparse matrices, so label encoding or target\n",
      "   encoding may be more practical in those cases.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 3 — Train / Test Split\n",
    "\n",
    "`train_test_split` randomly divides data into training and test sets.\n",
    "- `test_size=0.2` → 80% train, 20% test\n",
    "- Always set `random_state` for reproducibility"
   ],
   "id": "d0d561e8f5749ae5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.276326Z",
     "start_time": "2026-02-19T14:40:20.639502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset with 10 samples: 2 numeric features + 1 categorical feature\n",
    "X = ([12,300,'Red'],[11,280,'Red'],[15,264,'Black'],[9,230,'Blue'],\n",
    "     [25,459,'Black'],[12,400,'Red'],[42,355,'Blue'],[32,435,'Red'],\n",
    "     [22,564,'Black'],[21,231,'Red'])\n",
    "\n",
    "# Target labels\n",
    "y = [1,2,1,1,1,2,2,1,2,1]\n",
    "\n",
    "print(\"Full X:\", X)\n",
    "print(\"Full y:\", y)"
   ],
   "id": "2529997ef2044631",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full X: ([12, 300, 'Red'], [11, 280, 'Red'], [15, 264, 'Black'], [9, 230, 'Blue'], [25, 459, 'Black'], [12, 400, 'Red'], [42, 355, 'Blue'], [32, 435, 'Red'], [22, 564, 'Black'], [21, 231, 'Red'])\n",
      "Full y: [1, 2, 1, 1, 1, 2, 2, 1, 2, 1]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.289217Z",
     "start_time": "2026-02-19T14:40:33.283215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split: 80% train, 20% test, fixed seed for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=4\n",
    ")\n",
    "\n",
    "print(\"X_train (8 samples):\", X_train)\n",
    "print(\"\\nX_test  (2 samples):\", X_test)\n",
    "print(\"\\ny_train:\", y_train)\n",
    "print(\"y_test: \", y_test)"
   ],
   "id": "2f0736142d5dcd7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (8 samples): [[25, 459, 'Black'], [21, 231, 'Red'], [15, 264, 'Black'], [42, 355, 'Blue'], [12, 300, 'Red'], [11, 280, 'Red'], [12, 400, 'Red'], [32, 435, 'Red']]\n",
      "\n",
      "X_test  (2 samples): [[9, 230, 'Blue'], [22, 564, 'Black']]\n",
      "\n",
      "y_train: [1, 1, 1, 2, 1, 2, 2, 1]\n",
      "y_test:  [1, 2]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.338879Z",
     "start_time": "2026-02-19T14:40:33.335700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The test size (20%) → 2 out of 10 samples go to test, 8 to train.\n",
    "# random_state=4 ensures the same split every time you run the notebook.\n",
    "print(\"Train size:\", len(X_train), \"| Test size:\", len(X_test))"
   ],
   "id": "f1745d7c6cb0c0be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8 | Test size: 2\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "###  Exercise 3 — Inspect your split\n",
    "\n",
    "**Task:** Re-split with `test_size=0.3`, print sizes, count 1s and 2s in each subset."
   ],
   "id": "4648583f544081bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.380651Z",
     "start_time": "2026-02-19T14:40:33.365125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Split with test_size=0.3 (30% test, 70% train)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=4\n",
    ")\n",
    "\n",
    "# Step 2: Print sizes\n",
    "print(\"=== Sizes ===\")\n",
    "print(f\"X_train: {len(X_train2)} samples\")\n",
    "print(f\"X_test:  {len(X_test2)} samples\")\n",
    "print(f\"y_train: {len(y_train2)} labels\")\n",
    "print(f\"y_test:  {len(y_test2)} labels\")\n",
    "\n",
    "# Step 3: Count class labels\n",
    "n1_train = y_train2.count(1)\n",
    "n2_train = y_train2.count(2)\n",
    "n1_test  = y_test2.count(1)\n",
    "n2_test  = y_test2.count(2)\n",
    "\n",
    "print(\"\\n=== Class Counts ===\")\n",
    "print(f\"y_train  → class 1: {n1_train}, class 2: {n2_train}\")\n",
    "print(f\"y_test   → class 1: {n1_test},  class 2: {n2_test}\")\n",
    "\n",
    "# NOTE: Because the dataset is very small (10 samples), class balance\n",
    "# can shift noticeably between splits. In practice, use stratify=y\n",
    "# to preserve the original class proportions in both sets."
   ],
   "id": "5b342824de88ee4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sizes ===\n",
      "X_train: 7 samples\n",
      "X_test:  3 samples\n",
      "y_train: 7 labels\n",
      "y_test:  3 labels\n",
      "\n",
      "=== Class Counts ===\n",
      "y_train  → class 1: 4, class 2: 3\n",
      "y_test   → class 1: 2,  class 2: 1\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 4 — End-to-End Preprocessing (MinMaxScaler)\n",
    "\n",
    "**MinMaxScaler** rescales features to a fixed range (default [0, 1]):\n",
    "`x_scaled = (x - x_min) / (x_max - x_min)`\n",
    "\n",
    "**⚠️ Key rule — fit on train only:**\n",
    "Compute `x_min` and `x_max` from `X_train`, then apply to both `X_train` and `X_test`."
   ],
   "id": "5d7a920c38bcc166"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.412963Z",
     "start_time": "2026-02-19T14:40:33.401627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the fake_reg.csv dataset (gem stone measurements)\n",
    "rng = np.random.default_rng(SEED)\n",
    "n = 1000\n",
    "feature1 = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "feature2 = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "noise    = rng.normal(loc=0.0, scale=0.15, size=n)\n",
    "price    = 50_000 + 12_000*feature1 + 8_000*feature2 + 10_000*noise\n",
    "\n",
    "fake = pd.DataFrame({\"feature1\": feature1, \"feature2\": feature2, \"price\": price})\n",
    "fake.to_csv(\"fake_reg.csv\", index=False)\n",
    "print(\"Created fake_reg.csv with shape\", fake.shape)"
   ],
   "id": "7f60e482cf966cb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fake_reg.csv with shape (1000, 3)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.445581Z",
     "start_time": "2026-02-19T14:40:33.433929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('fake_reg.csv')\n",
    "df.head()"
   ],
   "id": "5cd6a8ced5953b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   feature1  feature2         price\n",
       "0  0.304717 -0.059283  52504.417319\n",
       "1 -1.039984 -0.729287  30687.078667\n",
       "2  0.750451 -0.414473  56340.644558\n",
       "3  0.940565  0.633910  66735.841242\n",
       "4 -1.951035  0.002993  24504.336744"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.304717</td>\n",
       "      <td>-0.059283</td>\n",
       "      <td>52504.417319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.039984</td>\n",
       "      <td>-0.729287</td>\n",
       "      <td>30687.078667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.750451</td>\n",
       "      <td>-0.414473</td>\n",
       "      <td>56340.644558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.940565</td>\n",
       "      <td>0.633910</td>\n",
       "      <td>66735.841242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.951035</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>24504.336744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.517507Z",
     "start_time": "2026-02-19T14:40:33.510305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract features (X) and target (y)\n",
    "X = df[['feature1', 'feature2']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Split 70% train / 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)   # (700, 2)\n",
    "print(\"X_test  shape:\", X_test.shape)    # (300, 2)\n",
    "print(\"y_train shape:\", y_train.shape)   # (700,)\n",
    "print(\"y_test  shape:\", y_test.shape)    # (300,)"
   ],
   "id": "756414f8042b4403",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (700, 2)\n",
      "X_test  shape: (300, 2)\n",
      "y_train shape: (700,)\n",
      "y_test  shape: (300,)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.632705Z",
     "start_time": "2026-02-19T14:40:33.628203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# CRITICAL: fit ONLY on training data (no data leakage)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both sets using the training statistics\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train after MinMaxScaler (first 3 rows):\")\n",
    "print(X_train[:3])\n",
    "print(\"\\nX_test after MinMaxScaler (first 3 rows):\")\n",
    "print(X_test[:3])"
   ],
   "id": "5bdf52e029ce88b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train after MinMaxScaler (first 3 rows):\n",
      "[[0.41336944 0.65206898]\n",
      " [0.50666961 0.39970742]\n",
      " [0.54779362 0.53040626]]\n",
      "\n",
      "X_test after MinMaxScaler (first 3 rows):\n",
      "[[0.4043664  0.04997697]\n",
      " [0.65259223 0.47939214]\n",
      " [0.68111567 0.44600036]]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "###  Exercise 4 — Check for data leakage\n",
    "\n",
    "**Task:** Verify `X_train` values are in [0,1], check `X_test` range, explain any out-of-range values."
   ],
   "id": "f7a416b93aeba67f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.707587Z",
     "start_time": "2026-02-19T14:40:33.703056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Min and max of X_train after scaling\n",
    "print(\"=== X_train (scaled) ===\")\n",
    "print(\"Per-column min:\", X_train.min(axis=0))   # Should both be exactly 0.0\n",
    "print(\"Per-column max:\", X_train.max(axis=0))   # Should both be exactly 1.0\n",
    "\n",
    "# Step 2: Min and max of X_test after scaling\n",
    "print(\"\\n=== X_test (scaled) ===\")\n",
    "print(\"Per-column min:\", X_test.min(axis=0))    # Could be slightly < 0\n",
    "print(\"Per-column max:\", X_test.max(axis=0))    # Could be slightly > 1\n",
    "\n",
    "# Step 3: Check if any test values fall outside [0, 1]\n",
    "outside = ((X_test < 0) | (X_test > 1)).any()\n",
    "print(\"\\nAny X_test values outside [0, 1]?\", outside)"
   ],
   "id": "14a939701f841075",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== X_train (scaled) ===\n",
      "Per-column min: [0. 0.]\n",
      "Per-column max: [1. 1.]\n",
      "\n",
      "=== X_test (scaled) ===\n",
      "Per-column min: [0.10513995 0.03028872]\n",
      "Per-column max: [0.96118634 0.98238243]\n",
      "\n",
      "Any X_test values outside [0, 1]? False\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:40:33.756085Z",
     "start_time": "2026-02-19T14:40:33.752271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Explanation ---\n",
    "explanation = \"\"\"\n",
    "Why can X_test values fall outside [0, 1]?\n",
    "\n",
    "1. MinMaxScaler learns x_min and x_max exclusively from X_train.\n",
    "   It scales values using: (x - train_min) / (train_max - train_min)\n",
    "\n",
    "2. If X_test contains values BELOW the training minimum, the scaled\n",
    "   value will be negative (< 0). If X_test values are ABOVE the\n",
    "   training maximum, the scaled value will exceed 1.\n",
    "\n",
    "3. This is CORRECT BEHAVIOUR and not a bug. The alternative — fitting\n",
    "   the scaler on the full dataset — would leak test-set information\n",
    "   into the preprocessing step, making model evaluation unreliable.\n",
    "   Slight out-of-range values on test data are acceptable.\n",
    "\"\"\"\n",
    "print(explanation)"
   ],
   "id": "e8bd6e2ce4b5d481",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why can X_test values fall outside [0, 1]?\n",
      "\n",
      "1. MinMaxScaler learns x_min and x_max exclusively from X_train.\n",
      "   It scales values using: (x - train_min) / (train_max - train_min)\n",
      "\n",
      "2. If X_test contains values BELOW the training minimum, the scaled\n",
      "   value will be negative (< 0). If X_test values are ABOVE the\n",
      "   training maximum, the scaled value will exceed 1.\n",
      "\n",
      "3. This is CORRECT BEHAVIOUR and not a bug. The alternative — fitting\n",
      "   the scaler on the full dataset — would leak test-set information\n",
      "   into the preprocessing step, making model evaluation unreliable.\n",
      "   Slight out-of-range values on test data are acceptable.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Summary — Key takeaways from Week 4\n",
    "\n",
    "| Concept | Tool | When to use |\n",
    "|---|---|---|\n",
    "| Standardisation | `StandardScaler` | KNN, SVM, PCA, gradient descent models |\n",
    "| Normalisation | `MinMaxScaler` | Neural networks, when bounded range needed |\n",
    "| Label encoding | `LabelEncoder` | Ordinal categories, tree models |\n",
    "| One-hot encoding | `OneHotEncoder` / `pd.get_dummies` | Nominal categories, linear/distance models |\n",
    "| Train/test split | `train_test_split` | Always — before any fitting! |\n",
    "\n",
    "### ⚠️ The golden rule\n",
    "**Fit preprocessing (scalers, encoders) on the training set only**, then transform both sets. Fitting on the full dataset causes **data leakage** and gives unreliable model evaluation."
   ],
   "id": "f0918c41b0a3fea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
